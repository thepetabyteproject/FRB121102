{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to read and process the mcmc outputs and return csv with parameters of all bursts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "import numpy as np\n",
    "from emcee.autocorr import AutocorrError\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import corner\n",
    "from chainconsumer import ChainConsumer\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radiometer(tsys, gain, bandwidth, time, npol=2):\n",
    "    return tsys / gain / np.sqrt(npol * bandwidth * time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"121102_paper/mcmc_final/\"\n",
    "cids = [x.split(\"/\")[-1][:-11] for x in glob.glob(PATH + \"*.h5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out of tau has to be used or not, remove samples if tau doesn't have to be used\n",
    "def samples2params(samples, meta_info):\n",
    "    fraction = np.sum(samples[:, 4] / samples[:, 5] < 6) / samples.shape[0]\n",
    "    print(f\"tau fraction {fraction:.3f}\")\n",
    "    if fraction > 0.5:\n",
    "        print(\"Using tau\")\n",
    "        use_tau = True\n",
    "        mask = samples[:, 4] / samples[:, 5] < 6\n",
    "        samples = samples[mask, :]\n",
    "    else:\n",
    "        use_tau = False\n",
    "        mask = samples[:, 4] / samples[:, 5] > 6\n",
    "        samples = np.delete(samples, 5, 1)\n",
    "        samples = samples[mask, :]\n",
    "\n",
    "    samples[:, 0] = (\n",
    "        meta_info[\"fileheader\"][\"fch1\"]\n",
    "        + samples[:, 0] * meta_info[\"fileheader\"][\"native_foff\"]\n",
    "    )\n",
    "    samples[:, 1] *= np.abs(meta_info[\"fileheader\"][\"native_foff\"])\n",
    "    samples[:, 2] *= (\n",
    "        radiometer(\n",
    "            27, 10, 2.355 * samples[:, 1] * 1e6, meta_info[\"fileheader\"][\"native_tsamp\"]\n",
    "        )\n",
    "        * 81.92e-3\n",
    "        / np.sqrt(64 - sum(meta_info[\"mask\"]))\n",
    "    )\n",
    "    samples[:, 3] = (\n",
    "        (samples[:, 3] + meta_info[\"nstart\"])\n",
    "        * meta_info[\"fileheader\"][\"native_tsamp\"]\n",
    "        / 3600\n",
    "        / 24\n",
    "    ) + meta_info[\"fileheader\"][\"tstart\"]\n",
    "    samples[:, 4] *= meta_info[\"fileheader\"][\"native_tsamp\"] * 1e3\n",
    "    if use_tau:\n",
    "        samples[:, 5] *= meta_info[\"fileheader\"][\"native_tsamp\"] * 1e3 * 81.92e-3\n",
    "        samples[:, 5] *= (1000 / meta_info[\"fileheader\"][\"fch1\"]) ** (-4)\n",
    "\n",
    "    param_list = [\n",
    "        r\"$\\mu_f$ (MHz)\",\n",
    "        r\"$\\sigma_f$ (MHz)\",\n",
    "        r\"$S$ (Jy ms)\",\n",
    "        r\"$\\mu_t$ (ms)\",\n",
    "        r\"$\\sigma_t$ (ms)\",\n",
    "    ]\n",
    "    if use_tau:\n",
    "        param_list += [r\"$\\tau$ (ms)\"]\n",
    "\n",
    "    param_list += [r\"DM (pc cm$^{-3}$)\"]\n",
    "    return samples, param_list, mask\n",
    "\n",
    "\n",
    "# read mcmc output h5 file, remove burnin, return samples for all components \n",
    "def get_chains_and_parameters(h5_filename, json_filename):\n",
    "    reader = emcee.backends.HDFBackend(h5_filename)\n",
    "\n",
    "    try:\n",
    "        tau = reader.get_autocorr_time()\n",
    "        burnin = int(2 * np.max(tau))\n",
    "        print(f\"burnin using tau is: {burnin}\")\n",
    "        samples = reader.get_chain(discard=burnin, flat=True)\n",
    "\n",
    "    except (AutocorrError, ValueError):\n",
    "        samples = reader.get_chain(discard=0, flat=True)\n",
    "        burnin = int(samples.shape[0] * 0.75)\n",
    "        samples = samples[burnin:, :]\n",
    "\n",
    "    print(\"burn-in: {0}\".format(burnin))\n",
    "    print(\"flat chain shape: {0}\".format(samples.shape))\n",
    "\n",
    "    with open(json_filename, \"r\") as f:\n",
    "        meta_info = json.loads(f.read())\n",
    "\n",
    "    if samples.shape[-1] == 7:\n",
    "        samples, param_list, _ = samples2params(samples, meta_info)\n",
    "        return samples, param_list\n",
    "    elif samples.shape[-1] == 14:\n",
    "        first_samples, first_params, mask1 = samples2params(samples[:, :7], meta_info)\n",
    "        second_samples, second_params, mask2 = samples2params(\n",
    "            samples[mask1, 7:], meta_info\n",
    "        )\n",
    "        param_list = []\n",
    "        for index, param in enumerate(first_params):\n",
    "            param_list.append(param + str(1))\n",
    "        for index, param in enumerate(second_params):\n",
    "            param_list.append(param + str(2))\n",
    "        return (np.hstack([first_samples[mask2], second_samples]), param_list)\n",
    "    else:\n",
    "        first_samples, first_params, mask1 = samples2params(samples[:, :7], meta_info)\n",
    "        second_samples, second_params, mask2 = samples2params(\n",
    "            samples[mask1, 7:14], meta_info\n",
    "        )\n",
    "        third_samples, third_params, mask3 = samples2params(\n",
    "            samples[mask2, 14:], meta_info\n",
    "        )\n",
    "        param_list = []\n",
    "        for index, param in enumerate(first_params):\n",
    "            param_list.append(param + str(1))\n",
    "        for index, param in enumerate(second_params):\n",
    "            param_list.append(param + str(2))\n",
    "        for index, param in enumerate(third_params):\n",
    "            param_list.append(param + str(3))\n",
    "        return (\n",
    "            np.hstack(\n",
    "                [\n",
    "                    first_samples[mask2, :][mask3, :],\n",
    "                    second_samples[mask3, :],\n",
    "                    third_samples,\n",
    "                ]\n",
    "            ),\n",
    "            param_list,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutocorrError_issue_list = []\n",
    "FileNotFoundError_issue_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_or_move_ahead(cand_id):\n",
    "    try:\n",
    "        h5_filename = PATH + cand_id + \"_samples.h5\"\n",
    "        json_filename = PATH + cand_id + \".json\"\n",
    "        samples, param_list = get_chains_and_parameters(h5_filename, json_filename)\n",
    "        if samples is not None:\n",
    "            a = np.quantile(samples, [0.16, 0.5, 0.84], axis=0)\n",
    "            median_values = a[1]\n",
    "            upper_errors = a[2] - a[1]\n",
    "            lower_error = a[1] - a[0]\n",
    "            value_dict = {}\n",
    "            for index, key in enumerate(param_list):\n",
    "                value_dict[key] = median_values[index]\n",
    "                value_dict[\"upper error\" + key] = upper_errors[index]\n",
    "                value_dict[\"lower error\" + key] = lower_error[index]\n",
    "            value_dict[\"cand_id\"] = cand_id\n",
    "            c = ChainConsumer()\n",
    "            c.add_chain(samples, parameters=param_list)\n",
    "            corner_plot_path = \"121102_paper/\"\n",
    "            corner_plot_path += \"mcmc_final/final_corner_plots/\"\n",
    "\n",
    "            fig = c.plotter.plot(\n",
    "                figsize=\"grow\",\n",
    "                filename=corner_plot_path + cand_id + \".png\",\n",
    "                display=False,\n",
    "            )\n",
    "            return value_dict\n",
    "    except FileNotFoundError as e:\n",
    "        return cand_id, \"FileNotFoundError\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "ans = Parallel(n_jobs=10)(delayed(try_or_move_ahead)(cid) for cid in tqdm(cids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutocorrError_issue_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileNotFoundError_issue_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_comp = []\n",
    "multi_comp = []\n",
    "\n",
    "for _dict in ans:\n",
    "    if \"$\\\\mu_f$ (MHz)1\" in _dict:\n",
    "        multi_comp.append(_dict)\n",
    "    else:\n",
    "        single_comp.append(_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(single_comp)\n",
    "df = df.sort_values(by=\"$\\mu_t$ (ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad cands to remove!\n",
    "try:\n",
    "    idx = df[df[\"cand_id\"].str.contains(\"snr_6.38018\")].index[0]\n",
    "    df = df.drop(idx, axis=0)\n",
    "except IndexError:\n",
    "    print(\"candidate not there\")\n",
    "\n",
    "try:\n",
    "    idx = df[df[\"cand_id\"].str.contains(\"snr_7.0830\")].index[0]\n",
    "    df = df.drop(idx, axis=0)\n",
    "except IndexError:\n",
    "    print(\"candidate not there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the reruns candidates?\n",
    "# PATH = \"/121102_paper/mcmc_final/reruns/\"\n",
    "# cids = [x.split(\"/\")[-1][:-11] for x in glob.glob(PATH + \"*.h5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"single_comp_all_topo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = pd.DataFrame(multi_comp).sort_values(by=\"$\\mu_t$ (ms)1\")\n",
    "df_mc.to_csv(\"multi_comp_all_topo.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
